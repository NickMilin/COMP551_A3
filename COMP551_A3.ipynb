{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceaRbug4qdRV"
      },
      "source": [
        "Diana Covaci, 261 086 280\n",
        "\n",
        "Nicholas Milin, 261 106 314\n",
        "\n",
        "Viktor Allais, 261 148 866"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Y05lOL2mperO",
        "outputId": "a11b7369-98c0-4311-c598-57a1d8d9a505"
      },
      "outputs": [],
      "source": [
        "!pip install -q numpy pandas matplotlib seaborn scikit-learn ucimlrepo\n",
        "!pip install -q torchvision\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import itertools\n",
        "from IPython.core.debugger import set_trace\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from itertools import combinations\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owbt6jrdfhIm"
      },
      "source": [
        "# Task 1: Acquire the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ0LKe2LfmsC",
        "outputId": "72649ad9-ca46-4bfc-b214-a1b9d7e8488d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n",
            "100.0%\n",
            "100.0%\n",
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.28604060411453247, 0.3530242443084717)\n",
            "(torch.Size([64, 784]), torch.Size([64]))\n",
            "784 10\n"
          ]
        }
      ],
      "source": [
        "# find mean & std for train_dataset\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
        "\n",
        "loader_iter = iter(loader)\n",
        "images, _ = next(loader_iter)\n",
        "mean_train = images.mean().item()\n",
        "std_train = images.std().item()\n",
        "print((mean_train, std_train))\n",
        "\n",
        "# normalize train_dataset & test_dataset\n",
        "mlp_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_train, std_train),\n",
        "    transforms.Lambda(lambda x:x.view(-1))\n",
        "])\n",
        "\n",
        "full_train_dataset_normalized = datasets.FashionMNIST(root='./data', train=True, download=True, transform=mlp_transform)\n",
        "test_dataset_normalized = datasets.FashionMNIST(root='./data', train=False, download=True, transform=mlp_transform)\n",
        "\n",
        "# split full train set into train and validation sets (80/20)\n",
        "train_size = int(0.8*len(full_train_dataset_normalized))\n",
        "val_size = len(full_train_dataset_normalized) - train_size\n",
        "train_dataset_normalized, val_dataset_normalized = random_split(full_train_dataset_normalized, [train_size, val_size])\n",
        "\n",
        "# create dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset_normalized, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = DataLoader(val_dataset_normalized, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset_normalized, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# sanity check\n",
        "x, y = next(iter(train_loader))\n",
        "print((x.shape, y.shape)) # expect [64, 784] and [64]\n",
        "input_size = x.shape[1]\n",
        "output_size = len(set(full_train_dataset_normalized.targets.numpy()))\n",
        "print(input_size, output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE_Kqx-dpZJn"
      },
      "source": [
        "# Task 2: Implement a Multilayer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ8_6TIyb0TM"
      },
      "source": [
        "## Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hbAzNQH8b1nB"
      },
      "outputs": [],
      "source": [
        "class relu:\n",
        "  def activation(self, x):\n",
        "    return np.maximum(0, x)\n",
        "  def derivative(self, x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "class leakyRelu:\n",
        "  def __init__(self, gamma=0.01):\n",
        "    self.gamma = gamma\n",
        "  def activation(self, x):\n",
        "    return np.where(x > 0, x, self.gamma * x)\n",
        "  def derivative(self, x):\n",
        "    return np.where(x > 0, 1, self.gamma)\n",
        "\n",
        "class sigmoid:\n",
        "  def activation(self, x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "  def derivative(self, x):\n",
        "    return self.activation(x) * (1 - self.activation(x))\n",
        "\n",
        "class tanh:\n",
        "  def activation(self, x):\n",
        "    return np.tanh(x)\n",
        "  def derivative(self, x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "class softmax:\n",
        "  def activation(self, x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    total = np.sum(exp_x, axis=1, keepdims=True)\n",
        "    return exp_x / total\n",
        "  def derivative(self, x):\n",
        "    return self.activation(x) * (1 - self.activation(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMFPFh8Mb43k"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CRKuxnhJpcS8"
      },
      "outputs": [],
      "source": [
        "def softmax(X):\n",
        "  exp_X = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "  return exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
        "\n",
        "# implementing the MLP class\n",
        "class MLP:\n",
        "\n",
        "  def __init__(self, activation_function, num_hidden_layers, units):\n",
        "    self.activation_function = activation_function\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.units = units\n",
        "\n",
        "    # initialize weights and biases\n",
        "    layer_sizes = [input_size] + units + [output_size]\n",
        "    self.L = num_hidden_layers + 1  # Number of layers (excluding input) (input is 0 indexed)\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "    self.activations = {}\n",
        "    self.z_values = {}\n",
        "\n",
        "    for i in range(len(layer_sizes)-1):\n",
        "      w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2/layer_sizes[i])\n",
        "      b = np.zeros((1, layer_sizes[i+1]))\n",
        "      self.weights.append(w)\n",
        "      self.biases.append(b)\n",
        "\n",
        "    self.grad_weights = {}\n",
        "    self.grad_biases = {}\n",
        "\n",
        "\n",
        "  def forward_propagation(self, X):\n",
        "    # Forward propagation through the network\n",
        "    self.activations[0] = X\n",
        "    previous_activation = X\n",
        "    # Hidden layers\n",
        "    for l in range(self.L):\n",
        "        W = self.weights[l]\n",
        "        b = self.biases[l]\n",
        "\n",
        "        Z = np.dot(previous_activation, W) + b\n",
        "        # Activation for Layer L (last layer)\n",
        "        if l == self.L - 1:\n",
        "          A = softmax(Z)\n",
        "        else:\n",
        "          A = self.activation_function.activation(Z)\n",
        "\n",
        "        self.z_values[l+1] = Z\n",
        "        self.activations[l+1] = A\n",
        "        previous_activation = A\n",
        "\n",
        "    # Sums to 1 as it is softmax\n",
        "    return previous_activation\n",
        "\n",
        "  def backward_propagation(self, AL, X, Y):\n",
        "    m = Y.shape[0]\n",
        "    num_layers = self.L\n",
        "    dZ = AL - Y\n",
        "\n",
        "    for l in reversed(range(num_layers)):\n",
        "      if l == 0:\n",
        "        previous_activation = X\n",
        "      else:\n",
        "        previous_activation = self.activations[l]\n",
        "      W = self.weights[l]\n",
        "\n",
        "      self.grad_weights[l] = np.dot(previous_activation.T, dZ) / m\n",
        "      self.grad_biases[l] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "      if l > 0:\n",
        "        # Compute gradient for previous layer\n",
        "        dA_prev = np.dot(dZ, W.T)\n",
        "        Z_prev = self.z_values[l]\n",
        "        dZ = dA_prev * self.activation_function.derivative(Z_prev)\n",
        "\n",
        "\n",
        "  def update_parameters(self):\n",
        "    for l in range(self.L):\n",
        "      self.weights[l] -= self.learning_rate * self.grad_weights[l]\n",
        "      self.biases[l] -= self.learning_rate * self.grad_biases[l]\n",
        "\n",
        "\n",
        "  def compute_loss(self, AL, Y):\n",
        "    \"\"\"Compute cross-entropy loss\"\"\"\n",
        "    m = Y.shape[0]\n",
        "\n",
        "    # Clip probabilities to avoid log(0)\n",
        "    epsilon = 1e-15\n",
        "    AL = np.clip(AL, epsilon, 1 - epsilon)\n",
        "\n",
        "    # Cross-entropy loss\n",
        "    cross_entropy = -np.sum(Y * np.log(AL)) / m\n",
        "\n",
        "    return cross_entropy\n",
        "\n",
        "  def compute_accuracy(self, true, pred):\n",
        "    predictions = np.argmax(pred, axis=1)\n",
        "    labels = np.argmax(true, axis=1)\n",
        "    accuracy = np.mean(predictions == labels)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "  def fit(self, train_loader, learning_rate, epochs):\n",
        "    self.learning_rate = learning_rate\n",
        "    for epoch in range(epochs):\n",
        "      epoch_loss = 0\n",
        "      n_batches = 0\n",
        "      epoch_accuracy = 0\n",
        "\n",
        "      for X_batch, y_batch in train_loader:\n",
        "        X_batch_np = X_batch.numpy()\n",
        "        y_batch_onehot = np.eye(output_size)[y_batch.numpy()] # One-hot encode labels\n",
        "\n",
        "        # Forward propagation\n",
        "        AL = self.forward_propagation(X_batch_np)\n",
        "\n",
        "        # Compute loss and accuracy\n",
        "        loss = self.compute_loss(AL, y_batch_onehot)\n",
        "        accuracy = self.compute_accuracy(y_batch_onehot, AL)\n",
        "        epoch_loss += loss\n",
        "        epoch_accuracy += accuracy\n",
        "        n_batches += 1\n",
        "\n",
        "        # Backward propagation\n",
        "        self.backward_propagation(AL, X_batch_np, y_batch_onehot)\n",
        "\n",
        "        # Update parameters\n",
        "        self.update_parameters()\n",
        "      print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/n_batches:.4f}, Accuracy: {epoch_accuracy/n_batches:.4f}\")\n",
        "\n",
        "  '''\n",
        "  def fit(self, X, y, learning_rate, epochs):\n",
        "    self.learning_rate = learning_rate\n",
        "    for epoch in range(epochs):\n",
        "      # Random permutation of indices\n",
        "      np.random.seed(42+epoch)\n",
        "      indices = np.random.permutation(N)\n",
        "      X_shuffled = X[indices]\n",
        "      y_shuffled = y[indices]\n",
        "\n",
        "      epoch_loss = 0\n",
        "      n_batches = 0\n",
        "      epoch_accuracy = 0\n",
        "\n",
        "      # Mini-batch gradient descent\n",
        "      for i in range(0, n_samples, batch_size):\n",
        "        # Get mini-batch\n",
        "        end_idx = min(i + batch_size, n_samples)\n",
        "        X_batch = X_shuffled[i:end_idx]\n",
        "        y_batch = y_shuffled[i:end_idx]\n",
        "\n",
        "        # Forward propagation\n",
        "        AL = self.forward_propagation(X_batch)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.compute_loss(AL, y_batch)\n",
        "        accuracy = self.compute_accuracy(y_batch, AL)\n",
        "        epoch_loss += loss\n",
        "        epoch_accuracy += accuracy\n",
        "        n_batches += 1\n",
        "\n",
        "        # Backward propagation\n",
        "        self.backward_propagation(AL, X_batch, y_batch)\n",
        "\n",
        "        # Update parameters\n",
        "        self.update_parameters()\n",
        "    return self\n",
        "'''\n",
        "  def predict(self, X):\n",
        "    \"\"\"Make predictions\"\"\"\n",
        "    AL = self.forward_propagation(X)\n",
        "    predictions = np.argmax(AL, axis=1)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4qhxp7XcCpJ"
      },
      "source": [
        "## Accuracy Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BzXIVrFpcEzx"
      },
      "outputs": [],
      "source": [
        "def evaluate_acc(true, pred):\n",
        "  return np.sum(true == pred) / len(true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEYFUetxZQWW"
      },
      "source": [
        "# Task 3: Run the experiments and report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MJw0infZcMV"
      },
      "source": [
        "## 3.1: Three different models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo6enaFDZhnW"
      },
      "source": [
        "### Model 1: No hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y0axLdhHZxkZ",
        "outputId": "35683f5e-2eaa-40d8-a383-e280f2494296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Loss: 0.6661, Accuracy: 0.7695\n",
            "Epoch 2/20, Loss: 0.5101, Accuracy: 0.8222\n",
            "Epoch 3/20, Loss: 0.4785, Accuracy: 0.8342\n",
            "Epoch 4/20, Loss: 0.4606, Accuracy: 0.8411\n",
            "Epoch 5/20, Loss: 0.4486, Accuracy: 0.8450\n",
            "Epoch 6/20, Loss: 0.4401, Accuracy: 0.8481\n",
            "Epoch 7/20, Loss: 0.4330, Accuracy: 0.8498\n",
            "Epoch 8/20, Loss: 0.4277, Accuracy: 0.8538\n",
            "Epoch 9/20, Loss: 0.4233, Accuracy: 0.8551\n",
            "Epoch 10/20, Loss: 0.4196, Accuracy: 0.8564\n",
            "Epoch 11/20, Loss: 0.4159, Accuracy: 0.8571\n",
            "Epoch 12/20, Loss: 0.4131, Accuracy: 0.8586\n",
            "Epoch 13/20, Loss: 0.4106, Accuracy: 0.8593\n",
            "Epoch 14/20, Loss: 0.4082, Accuracy: 0.8595\n",
            "Epoch 15/20, Loss: 0.4058, Accuracy: 0.8598\n",
            "Epoch 16/20, Loss: 0.4035, Accuracy: 0.8610\n",
            "Epoch 17/20, Loss: 0.4018, Accuracy: 0.8618\n",
            "Epoch 18/20, Loss: 0.4002, Accuracy: 0.8632\n",
            "Epoch 19/20, Loss: 0.3982, Accuracy: 0.8630\n",
            "Epoch 20/20, Loss: 0.3966, Accuracy: 0.8638\n"
          ]
        }
      ],
      "source": [
        "model1 = MLP(activation_function=relu(), num_hidden_layers=0, units=[])\n",
        "model1.fit(train_loader, 0.01, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8643125\n"
          ]
        }
      ],
      "source": [
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "\n",
        "for X_batch, y_batch in train_loader:\n",
        "    X_batch_np = X_batch.numpy()\n",
        "    y_batch_np = y_batch.numpy()\n",
        "\n",
        "    y_pred = model1.predict(X_batch_np)\n",
        "\n",
        "    all_y_true.extend(y_batch_np)\n",
        "    all_y_pred.extend(y_pred)\n",
        "\n",
        "# Convert lists to numpy arrays for evaluation\n",
        "all_y_true_np = np.array(all_y_true)\n",
        "all_y_pred_np = np.array(all_y_pred)\n",
        "\n",
        "print(evaluate_acc(all_y_true_np, all_y_pred_np))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx_xI3VyZlvZ"
      },
      "source": [
        "### Model 2: One hidden layers [256] ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI2S8KFTZ1xg",
        "outputId": "df1bbeea-abc9-4af8-db26-ff99596ce108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Loss: 0.6246, Accuracy: 0.7848\n",
            "Epoch 2/20, Loss: 0.4570, Accuracy: 0.8398\n",
            "Epoch 3/20, Loss: 0.4172, Accuracy: 0.8535\n",
            "Epoch 4/20, Loss: 0.3941, Accuracy: 0.8621\n",
            "Epoch 5/20, Loss: 0.3761, Accuracy: 0.8669\n",
            "Epoch 6/20, Loss: 0.3612, Accuracy: 0.8731\n",
            "Epoch 7/20, Loss: 0.3502, Accuracy: 0.8756\n",
            "Epoch 8/20, Loss: 0.3397, Accuracy: 0.8794\n",
            "Epoch 9/20, Loss: 0.3306, Accuracy: 0.8835\n",
            "Epoch 10/20, Loss: 0.3230, Accuracy: 0.8856\n",
            "Epoch 11/20, Loss: 0.3145, Accuracy: 0.8893\n",
            "Epoch 12/20, Loss: 0.3084, Accuracy: 0.8905\n",
            "Epoch 13/20, Loss: 0.3015, Accuracy: 0.8931\n",
            "Epoch 14/20, Loss: 0.2958, Accuracy: 0.8944\n",
            "Epoch 15/20, Loss: 0.2903, Accuracy: 0.8972\n",
            "Epoch 16/20, Loss: 0.2853, Accuracy: 0.8984\n",
            "Epoch 17/20, Loss: 0.2796, Accuracy: 0.9017\n",
            "Epoch 18/20, Loss: 0.2752, Accuracy: 0.9029\n",
            "Epoch 19/20, Loss: 0.2699, Accuracy: 0.9050\n",
            "Epoch 20/20, Loss: 0.2656, Accuracy: 0.9052\n",
            "0.9093125\n"
          ]
        }
      ],
      "source": [
        "model2 = MLP(activation_function=relu(), num_hidden_layers=1, units=[256])\n",
        "model2.fit(train_loader, 0.01, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "\n",
        "for X_batch, y_batch in train_loader:\n",
        "    X_batch_np = X_batch.numpy()\n",
        "    y_batch_np = y_batch.numpy()\n",
        "\n",
        "    y_pred = model2.predict(X_batch_np)\n",
        "\n",
        "    all_y_true.extend(y_batch_np)\n",
        "    all_y_pred.extend(y_pred)\n",
        "\n",
        "# Convert lists to numpy arrays for evaluation\n",
        "all_y_true_np = np.array(all_y_true)\n",
        "all_y_pred_np = np.array(all_y_pred)\n",
        "\n",
        "print(evaluate_acc(all_y_true_np, all_y_pred_np))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpQf4rmAZmXL"
      },
      "source": [
        "### Model 3: Two hidden layers [256,256] ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "exfQwNQeZbq0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Loss: 0.6057, Accuracy: 0.7884\n",
            "Epoch 2/20, Loss: 0.4331, Accuracy: 0.8464\n",
            "Epoch 3/20, Loss: 0.3900, Accuracy: 0.8627\n",
            "Epoch 4/20, Loss: 0.3647, Accuracy: 0.8712\n",
            "Epoch 5/20, Loss: 0.3446, Accuracy: 0.8775\n",
            "Epoch 6/20, Loss: 0.3311, Accuracy: 0.8819\n",
            "Epoch 7/20, Loss: 0.3176, Accuracy: 0.8866\n",
            "Epoch 8/20, Loss: 0.3057, Accuracy: 0.8904\n",
            "Epoch 9/20, Loss: 0.2956, Accuracy: 0.8941\n",
            "Epoch 10/20, Loss: 0.2862, Accuracy: 0.8981\n",
            "Epoch 11/20, Loss: 0.2779, Accuracy: 0.9008\n",
            "Epoch 12/20, Loss: 0.2701, Accuracy: 0.9037\n",
            "Epoch 13/20, Loss: 0.2616, Accuracy: 0.9055\n",
            "Epoch 14/20, Loss: 0.2558, Accuracy: 0.9090\n",
            "Epoch 15/20, Loss: 0.2488, Accuracy: 0.9112\n",
            "Epoch 16/20, Loss: 0.2424, Accuracy: 0.9135\n",
            "Epoch 17/20, Loss: 0.2351, Accuracy: 0.9164\n",
            "Epoch 18/20, Loss: 0.2307, Accuracy: 0.9179\n",
            "Epoch 19/20, Loss: 0.2238, Accuracy: 0.9207\n",
            "Epoch 20/20, Loss: 0.2187, Accuracy: 0.9218\n",
            "0.924625\n"
          ]
        }
      ],
      "source": [
        "model3 = MLP(activation_function=relu(), num_hidden_layers=2, units=[256,256])\n",
        "model3.fit(train_loader, 0.01, 20)\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "\n",
        "for X_batch, y_batch in train_loader:\n",
        "    X_batch_np = X_batch.numpy()\n",
        "    y_batch_np = y_batch.numpy()\n",
        "\n",
        "    y_pred = model3.predict(X_batch_np)\n",
        "\n",
        "    all_y_true.extend(y_batch_np)\n",
        "    all_y_pred.extend(y_pred)\n",
        "\n",
        "# Convert lists to numpy arrays for evaluation\n",
        "all_y_true_np = np.array(all_y_true)\n",
        "all_y_pred_np = np.array(all_y_pred)\n",
        "\n",
        "print(evaluate_acc(all_y_true_np, all_y_pred_np))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpmHuWF1ubYD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
